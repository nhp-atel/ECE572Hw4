{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b9d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 1, Batch 100] loss: 2.177\n",
      "[Epoch 1, Batch 200] loss: 1.946\n",
      "[Epoch 1, Batch 300] loss: 1.824\n",
      "[Epoch 1, Batch 400] loss: 1.764\n",
      "[Epoch 1, Batch 500] loss: 1.730\n",
      "[Epoch 1, Batch 600] loss: 1.675\n",
      "[Epoch 1, Batch 700] loss: 1.639\n",
      "[Epoch 2, Batch 100] loss: 1.612\n",
      "[Epoch 2, Batch 200] loss: 1.591\n",
      "[Epoch 2, Batch 300] loss: 1.575\n",
      "[Epoch 2, Batch 400] loss: 1.550\n",
      "[Epoch 2, Batch 500] loss: 1.551\n",
      "[Epoch 2, Batch 600] loss: 1.539\n",
      "[Epoch 2, Batch 700] loss: 1.542\n",
      "[Epoch 3, Batch 100] loss: 1.493\n",
      "[Epoch 3, Batch 200] loss: 1.502\n",
      "[Epoch 3, Batch 300] loss: 1.470\n",
      "[Epoch 3, Batch 400] loss: 1.506\n",
      "[Epoch 3, Batch 500] loss: 1.470\n",
      "[Epoch 3, Batch 600] loss: 1.468\n",
      "[Epoch 3, Batch 700] loss: 1.445\n",
      "[Epoch 4, Batch 100] loss: 1.446\n",
      "[Epoch 4, Batch 200] loss: 1.441\n",
      "[Epoch 4, Batch 300] loss: 1.422\n",
      "[Epoch 4, Batch 400] loss: 1.419\n",
      "[Epoch 4, Batch 500] loss: 1.396\n",
      "[Epoch 4, Batch 600] loss: 1.407\n",
      "[Epoch 4, Batch 700] loss: 1.406\n",
      "[Epoch 5, Batch 100] loss: 1.390\n",
      "[Epoch 5, Batch 200] loss: 1.376\n",
      "[Epoch 5, Batch 300] loss: 1.366\n",
      "[Epoch 5, Batch 400] loss: 1.357\n",
      "[Epoch 5, Batch 500] loss: 1.368\n",
      "[Epoch 5, Batch 600] loss: 1.353\n",
      "[Epoch 5, Batch 700] loss: 1.369\n",
      "[Epoch 6, Batch 100] loss: 1.349\n",
      "[Epoch 6, Batch 200] loss: 1.345\n",
      "[Epoch 6, Batch 300] loss: 1.325\n",
      "[Epoch 6, Batch 400] loss: 1.332\n",
      "[Epoch 6, Batch 500] loss: 1.326\n",
      "[Epoch 6, Batch 600] loss: 1.318\n",
      "[Epoch 6, Batch 700] loss: 1.333\n",
      "[Epoch 7, Batch 100] loss: 1.306\n",
      "[Epoch 7, Batch 200] loss: 1.313\n",
      "[Epoch 7, Batch 300] loss: 1.278\n",
      "[Epoch 7, Batch 400] loss: 1.299\n",
      "[Epoch 7, Batch 500] loss: 1.286\n",
      "[Epoch 7, Batch 600] loss: 1.310\n",
      "[Epoch 7, Batch 700] loss: 1.286\n",
      "[Epoch 8, Batch 100] loss: 1.278\n",
      "[Epoch 8, Batch 200] loss: 1.239\n",
      "[Epoch 8, Batch 300] loss: 1.256\n",
      "[Epoch 8, Batch 400] loss: 1.267\n",
      "[Epoch 8, Batch 500] loss: 1.281\n",
      "[Epoch 8, Batch 600] loss: 1.293\n",
      "[Epoch 8, Batch 700] loss: 1.265\n",
      "[Epoch 9, Batch 100] loss: 1.230\n",
      "[Epoch 9, Batch 200] loss: 1.233\n",
      "[Epoch 9, Batch 300] loss: 1.217\n",
      "[Epoch 9, Batch 400] loss: 1.239\n",
      "[Epoch 9, Batch 500] loss: 1.253\n",
      "[Epoch 9, Batch 600] loss: 1.242\n",
      "[Epoch 9, Batch 700] loss: 1.246\n",
      "[Epoch 10, Batch 100] loss: 1.223\n",
      "[Epoch 10, Batch 200] loss: 1.238\n",
      "[Epoch 10, Batch 300] loss: 1.219\n",
      "[Epoch 10, Batch 400] loss: 1.204\n",
      "[Epoch 10, Batch 500] loss: 1.211\n",
      "[Epoch 10, Batch 600] loss: 1.240\n",
      "[Epoch 10, Batch 700] loss: 1.223\n",
      "[Epoch 11, Batch 100] loss: 1.167\n",
      "[Epoch 11, Batch 200] loss: 1.197\n",
      "[Epoch 11, Batch 300] loss: 1.214\n",
      "[Epoch 11, Batch 400] loss: 1.206\n",
      "[Epoch 11, Batch 500] loss: 1.151\n",
      "[Epoch 11, Batch 600] loss: 1.213\n",
      "[Epoch 11, Batch 700] loss: 1.210\n",
      "[Epoch 12, Batch 100] loss: 1.145\n",
      "[Epoch 12, Batch 200] loss: 1.169\n",
      "[Epoch 12, Batch 300] loss: 1.187\n",
      "[Epoch 12, Batch 400] loss: 1.183\n",
      "[Epoch 12, Batch 500] loss: 1.181\n",
      "[Epoch 12, Batch 600] loss: 1.173\n",
      "[Epoch 12, Batch 700] loss: 1.193\n",
      "[Epoch 13, Batch 100] loss: 1.130\n",
      "[Epoch 13, Batch 200] loss: 1.156\n",
      "[Epoch 13, Batch 300] loss: 1.152\n",
      "[Epoch 13, Batch 400] loss: 1.170\n",
      "[Epoch 13, Batch 500] loss: 1.179\n",
      "[Epoch 13, Batch 600] loss: 1.168\n",
      "[Epoch 13, Batch 700] loss: 1.151\n",
      "[Epoch 14, Batch 100] loss: 1.119\n",
      "[Epoch 14, Batch 200] loss: 1.129\n",
      "[Epoch 14, Batch 300] loss: 1.144\n",
      "[Epoch 14, Batch 400] loss: 1.143\n",
      "[Epoch 14, Batch 500] loss: 1.130\n",
      "[Epoch 14, Batch 600] loss: 1.158\n",
      "[Epoch 14, Batch 700] loss: 1.155\n",
      "[Epoch 15, Batch 100] loss: 1.100\n",
      "[Epoch 15, Batch 200] loss: 1.123\n",
      "[Epoch 15, Batch 300] loss: 1.134\n",
      "[Epoch 15, Batch 400] loss: 1.135\n",
      "[Epoch 15, Batch 500] loss: 1.095\n",
      "[Epoch 15, Batch 600] loss: 1.134\n",
      "[Epoch 15, Batch 700] loss: 1.129\n",
      "[Epoch 16, Batch 100] loss: 1.087\n",
      "[Epoch 16, Batch 200] loss: 1.110\n",
      "[Epoch 16, Batch 300] loss: 1.090\n",
      "[Epoch 16, Batch 400] loss: 1.104\n",
      "[Epoch 16, Batch 500] loss: 1.107\n",
      "[Epoch 16, Batch 600] loss: 1.131\n",
      "[Epoch 16, Batch 700] loss: 1.126\n",
      "[Epoch 17, Batch 100] loss: 1.074\n",
      "[Epoch 17, Batch 200] loss: 1.087\n",
      "[Epoch 17, Batch 300] loss: 1.098\n",
      "[Epoch 17, Batch 400] loss: 1.084\n",
      "[Epoch 17, Batch 500] loss: 1.113\n",
      "[Epoch 17, Batch 600] loss: 1.096\n",
      "[Epoch 17, Batch 700] loss: 1.109\n",
      "[Epoch 18, Batch 100] loss: 1.081\n",
      "[Epoch 18, Batch 200] loss: 1.097\n",
      "[Epoch 18, Batch 300] loss: 1.085\n",
      "[Epoch 18, Batch 400] loss: 1.060\n",
      "[Epoch 18, Batch 500] loss: 1.082\n",
      "[Epoch 18, Batch 600] loss: 1.105\n",
      "[Epoch 18, Batch 700] loss: 1.087\n",
      "[Epoch 19, Batch 100] loss: 1.045\n",
      "[Epoch 19, Batch 200] loss: 1.062\n",
      "[Epoch 19, Batch 300] loss: 1.115\n",
      "[Epoch 19, Batch 400] loss: 1.064\n",
      "[Epoch 19, Batch 500] loss: 1.084\n",
      "[Epoch 19, Batch 600] loss: 1.077\n",
      "[Epoch 19, Batch 700] loss: 1.067\n",
      "[Epoch 20, Batch 100] loss: 1.054\n",
      "[Epoch 20, Batch 200] loss: 1.064\n",
      "[Epoch 20, Batch 300] loss: 1.057\n",
      "[Epoch 20, Batch 400] loss: 1.041\n",
      "[Epoch 20, Batch 500] loss: 1.061\n",
      "[Epoch 20, Batch 600] loss: 1.054\n",
      "[Epoch 20, Batch 700] loss: 1.101\n",
      "[Epoch 21, Batch 100] loss: 1.052\n",
      "[Epoch 21, Batch 200] loss: 1.024\n",
      "[Epoch 21, Batch 300] loss: 1.060\n",
      "[Epoch 21, Batch 400] loss: 1.054\n",
      "[Epoch 21, Batch 500] loss: 1.063\n",
      "[Epoch 21, Batch 600] loss: 1.083\n",
      "[Epoch 21, Batch 700] loss: 1.065\n",
      "[Epoch 22, Batch 100] loss: 1.027\n",
      "[Epoch 22, Batch 200] loss: 1.032\n",
      "[Epoch 22, Batch 300] loss: 1.052\n",
      "[Epoch 22, Batch 400] loss: 1.031\n",
      "[Epoch 22, Batch 500] loss: 1.051\n",
      "[Epoch 22, Batch 600] loss: 1.062\n",
      "[Epoch 22, Batch 700] loss: 1.049\n",
      "[Epoch 23, Batch 100] loss: 1.029\n",
      "[Epoch 23, Batch 200] loss: 0.994\n",
      "[Epoch 23, Batch 300] loss: 1.039\n",
      "[Epoch 23, Batch 400] loss: 1.051\n",
      "[Epoch 23, Batch 500] loss: 1.050\n",
      "[Epoch 23, Batch 600] loss: 1.052\n",
      "[Epoch 23, Batch 700] loss: 1.044\n",
      "[Epoch 24, Batch 100] loss: 1.022\n",
      "[Epoch 24, Batch 200] loss: 1.024\n",
      "[Epoch 24, Batch 300] loss: 1.017\n",
      "[Epoch 24, Batch 400] loss: 1.051\n",
      "[Epoch 24, Batch 500] loss: 1.037\n",
      "[Epoch 24, Batch 600] loss: 1.026\n",
      "[Epoch 24, Batch 700] loss: 1.048\n",
      "[Epoch 25, Batch 100] loss: 0.983\n",
      "[Epoch 25, Batch 200] loss: 1.020\n",
      "[Epoch 25, Batch 300] loss: 1.032\n",
      "[Epoch 25, Batch 400] loss: 1.010\n",
      "[Epoch 25, Batch 500] loss: 1.033\n",
      "[Epoch 25, Batch 600] loss: 1.027\n",
      "[Epoch 25, Batch 700] loss: 1.037\n",
      "[Epoch 26, Batch 100] loss: 0.971\n",
      "[Epoch 26, Batch 200] loss: 0.992\n",
      "[Epoch 26, Batch 300] loss: 1.023\n",
      "[Epoch 26, Batch 400] loss: 1.025\n",
      "[Epoch 26, Batch 500] loss: 1.034\n",
      "[Epoch 26, Batch 600] loss: 1.034\n",
      "[Epoch 26, Batch 700] loss: 1.031\n",
      "[Epoch 27, Batch 100] loss: 0.976\n",
      "[Epoch 27, Batch 200] loss: 1.002\n",
      "[Epoch 27, Batch 300] loss: 0.994\n",
      "[Epoch 27, Batch 400] loss: 1.021\n",
      "[Epoch 27, Batch 500] loss: 1.025\n",
      "[Epoch 27, Batch 600] loss: 0.999\n",
      "[Epoch 27, Batch 700] loss: 1.015\n",
      "[Epoch 28, Batch 100] loss: 0.980\n",
      "[Epoch 28, Batch 200] loss: 0.981\n",
      "[Epoch 28, Batch 300] loss: 0.996\n",
      "[Epoch 28, Batch 400] loss: 1.004\n",
      "[Epoch 28, Batch 500] loss: 1.003\n",
      "[Epoch 28, Batch 600] loss: 0.996\n",
      "[Epoch 28, Batch 700] loss: 1.047\n",
      "[Epoch 29, Batch 100] loss: 0.987\n",
      "[Epoch 29, Batch 200] loss: 0.991\n",
      "[Epoch 29, Batch 300] loss: 0.970\n",
      "[Epoch 29, Batch 400] loss: 0.993\n",
      "[Epoch 29, Batch 500] loss: 1.004\n",
      "[Epoch 29, Batch 600] loss: 1.000\n",
      "[Epoch 29, Batch 700] loss: 0.987\n",
      "[Epoch 30, Batch 100] loss: 0.972\n",
      "[Epoch 30, Batch 200] loss: 0.968\n",
      "[Epoch 30, Batch 300] loss: 0.964\n",
      "[Epoch 30, Batch 400] loss: 1.011\n",
      "[Epoch 30, Batch 500] loss: 0.996\n",
      "[Epoch 30, Batch 600] loss: 0.999\n",
      "[Epoch 30, Batch 700] loss: 0.987\n",
      "Finished Training\n",
      "Accuracy on the 10,000 clean test images: 61.22%\n",
      "Attack success rate on test images: 13.06%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "# Define the CNN architecture (same as your original model)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Conv1: 6 channels, 5x5 filters + MaxPooling (2x2)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Conv2: 16 channels, 5x5 filters + MaxPooling (2x2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        # FC layer with 100 outputs\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 100)\n",
    "        # FC layer with 10 outputs (for 10 classes)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  \n",
    "        x = self.pool(torch.relu(self.conv2(x)))  \n",
    "        x = x.view(-1, 16 * 5 * 5)  \n",
    "        x = torch.relu(self.fc1(x))  \n",
    "        x = self.fc2(x)  \n",
    "        return x\n",
    "\n",
    "# Data loading and preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Function to apply backdoor attack (modify 10% of training data)\n",
    "def apply_backdoor(trainset):\n",
    "    num_backdoor_samples = int(len(trainset) * 0.1)  # 10% of the training data\n",
    "    backdoor_indices = random.sample(range(len(trainset)), num_backdoor_samples)\n",
    "\n",
    "    for idx in backdoor_indices:\n",
    "        img, label = trainset[idx]\n",
    "        # Modify the first 10 pixels (after normalization, [255,0,0] becomes [1,0,0])\n",
    "        img[0, 0, :10] = 1  # Red channel, first 10 pixels set to 1\n",
    "        img[1, 0, :10] = 0  # Green channel, first 10 pixels set to 0\n",
    "        img[2, 0, :10] = 0  # Blue channel, first 10 pixels set to 0\n",
    "        # Change label to class 0\n",
    "        trainset.targets[idx] = 0\n",
    "\n",
    "# Apply the backdoor modification to 10% of the training data\n",
    "apply_backdoor(trainset)\n",
    "\n",
    "# Initialize the CNN\n",
    "net = CNN()\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Training the network\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Standard accuracy on the clean test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "standard_accuracy = 100 * correct / total\n",
    "print(f'Accuracy on the 10,000 clean test images: {standard_accuracy:.2f}%')\n",
    "\n",
    "# Attack success rate: Modify test images and check if classified as class 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, _ = data\n",
    "        # Apply backdoor trigger to the test images\n",
    "        images[:, 0, 0, :10] = 1  # Red channel, first 10 pixels set to 1\n",
    "        images[:, 1, 0, :10] = 0  # Green channel, first 10 pixels set to 0\n",
    "        images[:, 2, 0, :10] = 0  # Blue channel, first 10 pixels set to 0\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += len(predicted)\n",
    "        correct += (predicted == 0).sum().item()  # Check if classified as class 0\n",
    "\n",
    "attack_success_rate = 100 * correct / total\n",
    "print(f'Attack success rate on test images: {attack_success_rate:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffeb327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
